{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f724d80",
   "metadata": {},
   "source": [
    "# **MLP** (Multi-Layer Perceptron)\n",
    "<img src=\"docs/images/image-5.png\" alt=\"mlp\" width=\"550\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5223eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as m\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0214277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "\n",
    "    mu = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    std = np.where(std == 0, 1, std)\n",
    "\n",
    "    XS = (X - mu) / std\n",
    "\n",
    "    return XS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fa36cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raulserrano/Code/ml_projects/perceptron/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"uciml/pima-indians-diabetes-database\")\n",
    "pima = pd.read_csv(f\"{path}/diabetes.csv\")\n",
    "pima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d233f0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pima.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e2d1c",
   "metadata": {},
   "source": [
    "- EJEMPLO GPT para visualizar matrices de parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "313301af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de entrada X:\n",
      "     x1    x2    x3    x4    x5    x6    x7    x8\n",
      "0 -1.19  1.54  1.38 -0.03 -1.71 -0.22  0.26 -0.42\n",
      "1  1.79 -0.00 -0.46  0.13 -1.69  0.65 -1.03  0.02\n",
      "2 -1.21  0.40  0.94  2.83 -0.73 -0.90  0.99  1.14\n",
      "3 -0.17 -0.98 -0.57 -1.20 -0.61 -0.37  0.08  0.53\n",
      "4 -0.12  1.32  0.60 -1.41 -0.75 -0.92 -0.90  1.30 \n",
      "\n",
      "Pesos W1 (4 √ó 8):\n",
      "      x1    x2    x3    x4    x5    x6    x7    x8\n",
      "h1  1.94 -0.14 -0.33  1.03  0.89 -0.21  1.50  0.37\n",
      "h2  1.49 -0.75 -1.80 -0.64 -1.62 -0.54 -0.26 -0.92\n",
      "h3  1.23 -0.65  1.02 -0.44  1.57 -0.39  0.62  0.83\n",
      "h4 -0.81 -0.44  0.81 -0.93 -0.38 -0.46  1.40 -2.08 \n",
      "\n",
      "Bias b1: [ 2.04 -0.01  0.78 -0.35] \n",
      "\n",
      "Pesos W2 (2 √ó 4):\n",
      "      h1    h2    h3    h4\n",
      "o1  0.09 -0.72  1.39  0.78\n",
      "o2  1.47 -1.80  1.88  0.03 \n",
      "\n",
      "Bias b2: [1.89 0.97]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ejemplo: 5 instancias √ó 8 features\n",
    "X = np.random.randn(5, 8).round(2)\n",
    "df_X = pd.DataFrame(X, columns=[f\"x{i+1}\" for i in range(8)])\n",
    "print(\"Matriz de entrada X:\")\n",
    "print(df_X, \"\\n\")\n",
    "\n",
    "# Pesos capa oculta: supongamos h = 4 neuronas\n",
    "W1 = np.random.randn(4, 8).round(2) \n",
    "df_W1 = pd.DataFrame(W1,\n",
    "                     index=[f\"h{j+1}\" for j in range(4)],\n",
    "                     columns=[f\"x{i+1}\" for i in range(8)])\n",
    "print(\"Pesos W1 (4 √ó 8):\")\n",
    "print(df_W1, \"\\n\")\n",
    "\n",
    "# Bias de la capa oculta\n",
    "b1 = np.random.randn(4).round(2)\n",
    "print(\"Bias b1:\", b1, \"\\n\")\n",
    "\n",
    "# Pesos capa salida: 2 neuronas √ó h\n",
    "W2 = np.random.randn(2, 4).round(2)\n",
    "df_W2 = pd.DataFrame(W2,\n",
    "                     index=[f\"o{k+1}\" for k in range(2)],\n",
    "                     columns=[f\"h{j+1}\" for j in range(4)])\n",
    "print(\"Pesos W2 (2 √ó 4):\")\n",
    "print(df_W2, \"\\n\")\n",
    "\n",
    "# Bias de la capa salida\n",
    "b2 = np.random.randn(2).round(2)\n",
    "print(\"Bias b2:\", b2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee04af",
   "metadata": {},
   "source": [
    "## DESDE 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ee9a7",
   "metadata": {},
   "source": [
    "## **ARQUITECTURA**\n",
    "\n",
    "* **Entry: 8** atributos\n",
    "* **Capas ocultas (H): 1**\n",
    "* **Neuronas** Capa **H: 3**\n",
    "* **Neuronas** Capa **Salida (O) = 2**\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"docs/images/x1.png\" alt=\"perceptron\" width=\"500\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24a601d",
   "metadata": {},
   "source": [
    "## **DISPOSICI√ìN MATRICIAL OPTIMA**\n",
    "\n",
    "\n",
    "### 1Ô∏è‚É£ **Entrada**\n",
    "\n",
    "Cada dato de entrada (una instancia):\n",
    "\n",
    "$$\n",
    "x = \\begin{bmatrix} x_1 & x_2 & x_3 & x_4 \\end{bmatrix}_{(1,4)}\n",
    "$$\n",
    "\n",
    "Si usas un batch de tama√±o $N$:\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{N \\times 4}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **Capa oculta**\n",
    "\n",
    "#### ¬∑ **Pesos**\n",
    "\n",
    "$$\n",
    "W^{(1)} \\in \\mathbb{R}^{3 \\times 4}\n",
    "$$\n",
    "\n",
    "#### ¬∑ **Bias**\n",
    "\n",
    "$$\n",
    "b^{(1)} \\in \\mathbb{R}^{1 \\times 3}\n",
    "$$\n",
    "\n",
    "#### ¬∑ **C√°lculo de preactivaci√≥n (z)**\n",
    "\n",
    "$$\n",
    "Z^{(1)} = X \\cdot W^{(1)^T} + b^{(1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^{(1)} \\in \\mathbb{R}^{N \\times 3}\n",
    "$$\n",
    "\n",
    "#### ¬∑ **C√°lculo de la salida (activaci√≥n)**\n",
    "\n",
    "$$\n",
    "H^{(1)} = f(Z^{(1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "H^{(1)} \\in \\mathbb{R}^{N \\times 3}\n",
    "$$\n",
    "\n",
    "Aqu√≠ $f$ puede ser, por ejemplo, ReLU o Sigmoid, seg√∫n la arquitectura.\n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Capa de salida**\n",
    "\n",
    "  #### ¬∑ **Pesos**\n",
    "\n",
    "$$\n",
    "W^{(2)} \\in \\mathbb{R}^{2 \\times 3}\n",
    "$$\n",
    "\n",
    "#### ¬∑ **Bias**\n",
    "\n",
    "$$\n",
    "b^{(2)} \\in \\mathbb{R}^{1 \\times 2}\n",
    "$$\n",
    "\n",
    "#### ¬∑ **C√°lculo de preactivaci√≥n (z)**\n",
    "\n",
    "$$\n",
    "Z^{(2)} = H^{(1)} \\cdot W^{(2)^T} + b^{(2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^{(2)} \\in \\mathbb{R}^{N \\times 2}\n",
    "$$\n",
    "\n",
    "#### ¬∑ **C√°lculo de la salida final (activaci√≥n)**\n",
    "\n",
    "$$\n",
    "Y = g(Z^{(2)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y \\in \\mathbb{R}^{N \\times 2}\n",
    "$$\n",
    "\n",
    "Aqu√≠ $g$ puede ser, por ejemplo:\n",
    "\n",
    "* Softmax si es clasificaci√≥n multiclase\n",
    "* Sigmoid si es binaria\n",
    "* Identidad (sin activaci√≥n) si es regresi√≥n\n",
    "\n",
    "---\n",
    "\n",
    "### Resumen de dimensiones\n",
    "\n",
    "| Elemento                     | Forma  |\n",
    "| ---------------------------- | ------ |\n",
    "| Entrada $X$                  | (N, 4) |\n",
    "| Pesos $W^{(1)}$              | (3, 4) |\n",
    "| Bias $b^{(1)}$               | (1, 3) |\n",
    "| Preactivaci√≥n $Z^{(1)}$      | (N, 3) |\n",
    "| Salida capa oculta $H^{(1)}$ | (N, 3) |\n",
    "| Pesos $W^{(2)}$              | (2, 3) |\n",
    "| Bias $b^{(2)}$               | (1, 2) |\n",
    "| Preactivaci√≥n $Z^{(2)}$      | (N, 2) |\n",
    "| Salida final $Y$             | (N, 2) |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50738980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.63994726,  0.84832379,  0.14964075, ...,  0.20401277,\n",
       "         0.46849198,  1.4259954 ],\n",
       "       [-0.84488505, -1.12339636, -0.16054575, ..., -0.68442195,\n",
       "        -0.36506078, -0.19067191],\n",
       "       [ 1.23388019,  1.94372388, -0.26394125, ..., -1.10325546,\n",
       "         0.60439732, -0.10558415],\n",
       "       ...,\n",
       "       [ 0.3429808 ,  0.00330087,  0.14964075, ..., -0.73518964,\n",
       "        -0.68519336, -0.27575966],\n",
       "       [-0.84488505,  0.1597866 , -0.47073225, ..., -0.24020459,\n",
       "        -0.37110101,  1.17073215],\n",
       "       [-0.84488505, -0.8730192 ,  0.04624525, ..., -0.20212881,\n",
       "        -0.47378505, -0.87137393]], shape=(768, 8))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = pima['Outcome']\n",
    "X = pima.drop('Outcome', axis=1).values\n",
    "X = standardize(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e46302",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_len = X.shape[1]\n",
    "H_len = 3\n",
    "O_len = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0930ab",
   "metadata": {},
   "source": [
    "## **INICIALIZACION DE PESOS Y BIAS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a7022",
   "metadata": {},
   "source": [
    "### Inicializaci√≥n **Glorot** (Xavier)\n",
    "\n",
    "##### Para **inicializar los pesos** de cada capa hay que cuidar el **rango y la distribucion** de los mismos para evitar desvanecimiento de pesos y gradientes, para ello se inicializan aleatoriamente pero teniendo en cuenta que:\n",
    "\n",
    "`np.random.randn()` produce una **normal est√°ndar**: media‚ÄØ0 y varianza‚ÄØ1.\n",
    "Si la dejamos tal cual y la red tiene muchas entradas por neurona, al multiplicar por la matriz de pesos el resultado se convierte en una **suma de muchos t√©rminos independientes**:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{n_{\\text{in}}} w_i\\,x_i\n",
    "$$\n",
    "\n",
    "* Si $w_i$ y $x_i$ tienen varianza 1, entonces\n",
    "  $\\operatorname{Var}(z) = n_{\\text{in}}\\!\\cdot\\!1\\cdot 1 = n_{\\text{in}}$.\n",
    "  Con decenas o centenas de entradas, la varianza de $z$ se dispara ‚áí activaciones enormes ‚áí sigmoides saturadas ‚áí gradientes \\~‚ÄØ0 (vanishing).\n",
    "  \n",
    "* Si hacemos lo contrario (pesos demasiado peque√±os), la se√±al se aten√∫a ‚áí gradientes diminutos.\n",
    "\n",
    "La soluci√≥n es **imponer** una varianza m√°s baja a los pesos.\n",
    "Tomamos los n√∫meros de $\\mathcal N(0,1)$ y **los multiplicamos por $\\sqrt{\\text{Var deseada}}$**.  Escalar por la desviaci√≥n est√°ndar es la forma directa de transformar una normal est√°ndar en otra normal con la varianza que queremos:\n",
    "\n",
    "$$\n",
    "w = \\underbrace{\\mathcal N(0,1)}_{\\text{salida de randn}} \\times \\sqrt{\\operatorname{Var}_{\\text{deseada}}}\n",
    "$$\n",
    "\n",
    "\n",
    "Para una capa con  \n",
    "- $n_{\\text{in}}$: n√∫mero de neuronas que **entran**  \n",
    "- $n_{\\text{out}}$: n√∫mero de neuronas que **salen**\n",
    "\n",
    "se fija la varianza de los pesos como\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(W)=\\frac{2}{n_{\\text{in}}+n_{\\text{out}}}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "w = \\underbrace{\\mathcal N(0,1)}_{\\text{salida de randn}} \\times \\sqrt{\\operatorname{Var}_{\\text{(W)}}}\n",
    "$$\n",
    "\n",
    "Dos implementaciones habituales:\n",
    "\n",
    "| Distribuci√≥n | F√≥rmula de muestreo |\n",
    "|--------------|--------------------|\n",
    "| Normal | $W_{ij}\\sim\\mathcal{N}\\!\\Bigl(0,\\; \\frac{2}{n_{\\text{in}}+n_{\\text{out}}}\\Bigr)$ |\n",
    "\n",
    "---\n",
    "> **Python (normal):**\n",
    "> ```python\n",
    "> W = np.random.randn(n_in, n_out) * np.sqrt(2 / (n_in + n_out))\n",
    "> ```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70b19fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.set_printoptions(\n",
    "    precision=3,      # decimales\n",
    "    suppress=True,    # evita notaci√≥n cient√≠fica\n",
    "    linewidth=120,    # ancho de l√≠nea antes de saltar\n",
    "    formatter={'float': '{:7.3f}'.format}  # ancho fijo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa9227a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **1. H**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb49edb",
   "metadata": {},
   "source": [
    "#### **W_xh**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85caa677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      "[[  0.340  -0.182  -0.133   0.196  -0.368  -0.010   0.528  -0.353]\n",
      " [  0.305  -0.036   0.014   0.616   0.046   0.539  -0.277  -0.387]\n",
      " [ -0.160   0.469  -0.054  -0.745  -0.119   0.060   0.057   0.813]]\n",
      "\n",
      "Traspuesta\n",
      "[[  0.340   0.305  -0.160]\n",
      " [ -0.182  -0.036   0.469]\n",
      " [ -0.133   0.014  -0.054]\n",
      " [  0.196   0.616  -0.745]\n",
      " [ -0.368   0.046  -0.119]\n",
      " [ -0.010   0.539   0.060]\n",
      " [  0.528  -0.277   0.057]\n",
      " [ -0.353  -0.387   0.813]]\n"
     ]
    }
   ],
   "source": [
    "Wxh = np.random.randn(H_len, entry_len) * np.sqrt(2 / (8 + 3))\n",
    "print('Original')\n",
    "print(Wxh)\n",
    "print('\\nTraspuesta')\n",
    "print(Wxh.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9019c85",
   "metadata": {},
   "source": [
    "#### **B_h**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9697e01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.000,   0.000,   0.000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bh = np.zeros(H_len)\n",
    "Bh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20769f90",
   "metadata": {},
   "source": [
    "### **2. O**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f74bb7",
   "metadata": {},
   "source": [
    "#### **W_ho**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17f234ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      "[[ -0.810  -0.035  -0.113]\n",
      " [ -0.176   0.546   0.355]]\n",
      "\n",
      "Traspuesta\n",
      "[[ -0.810  -0.176]\n",
      " [ -0.035   0.546]\n",
      " [ -0.113   0.355]]\n"
     ]
    }
   ],
   "source": [
    "Who = np.random.randn(O_len, H_len) * np.sqrt(2 / (3 + 2))\n",
    "print('Original')\n",
    "print(Who)\n",
    "print('\\nTraspuesta')\n",
    "print(Who.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981d42c",
   "metadata": {},
   "source": [
    "#### **B_o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a84c2713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.000,   0.000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bo = np.zeros(O_len)\n",
    "Bo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91888af5",
   "metadata": {},
   "source": [
    "## **PROCESO LINEAL PARA CADA INSTANCIA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5b44a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1 = np.array(X[0])\n",
    "X_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af36b220",
   "metadata": {},
   "source": [
    "### FORMULAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c35844",
   "metadata": {},
   "source": [
    "### * **Z**\n",
    "\n",
    "$$\n",
    "Z^{(N)} = X \\cdot W^{(N)^T} + b^{(N)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a937b208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.217,   0.122,   0.892])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "X_1 = (1, 8)\n",
    "Wxh = (3, 8) -->  Wxh.T = (8, 3)\n",
    "Bh = (1, 3)\n",
    "'''\n",
    "#   X_1 @ Wxh = (1,3)\n",
    "\n",
    "Zh = (X_1 @ Wxh.T) + Bh\n",
    "Zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bbe6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z(X, W, B):\n",
    "\n",
    "    z = (X @ W.T) + B\n",
    "\n",
    "    return z  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2233aad6",
   "metadata": {},
   "source": [
    "### * Sigmoid **ùõî(z)**\n",
    "\n",
    "$$\n",
    "ùõî(z)=\\dfrac{1}{1+e^{-z}}  \n",
    "$$\n",
    "\n",
    "Version **equivalente para evitar overflow en la sigmoide:**\n",
    "\n",
    "$$\n",
    "\\sigma(z)=\n",
    "\\begin{cases}\n",
    "\\dfrac{1}{1+e^{-z}}, & z\\ge 0,\\\\[8pt]\n",
    "\\dfrac{e^{\\,z}}{1+e^{\\,z}}, & z<0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* Para $z\\ge 0$ el t√©rmino $e^{-z}$ es peque√±o y no explota.\n",
    "* Para $z<0$ se usa $e^{\\,z}$, que es diminuto (no desborda), y la fracci√≥n es algebraicamente equivalente a la sigmoide original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bb8a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMULA SIGMOIDE ESTABLE PERCEPTRON, PARA Z ESCALAR\n",
    "def sigmoid_esc(Z):\n",
    "\n",
    "        # f = 1 / (1 + m.exp(-NET))      <--   overflows if NET > abs(700)\n",
    "        # return f\n",
    "\n",
    "        if Z >= 0:\n",
    "            return 1 / (1 + m.exp(-Z))\n",
    "        else:\n",
    "            ez = m.exp(Z)        #  z is negative ‚Üí ez ‚â™ 1\n",
    "            return ez / (1 + ez)\n",
    "\n",
    "\n",
    "# FORMULA SIGMOIDE ESTABLE PARA Z\n",
    "def sigmoid(Z):\n",
    "\n",
    "    # # 1) Condici√≥n booleana para cada elemento\n",
    "    # cond = Z >= 0\n",
    "\n",
    "    # # 2) Dos expresiones, una para Z >= 0 y otra para Z < 0\n",
    "    # pos = 1 / (1 + np.exp(-Z))          # parte 'positiva'\n",
    "    # neg = np.exp(Z) / (1 + np.exp(Z))   # parte 'negativa'\n",
    "\n",
    "    # # 3) Mezclamos los dos resultados seg√∫n la condici√≥n\n",
    "    # result = np.where(cond, pos, neg)\n",
    "\n",
    "    # # 4) Devolvemos la matriz/vector escalar con la sigmoide\n",
    "    # return result\n",
    "    \n",
    "    result = np.where(\n",
    "        Z >= 0,                      # Pej:  # [False False  True  True  True]\n",
    "        1 / (1 + np.exp(-Z)),        # Array con la fx para Z >= 0 (A)\n",
    "        np.exp(Z) / (1 + np.exp(Z))  # Array con la fx para Z < 0  (B)\n",
    "        )                              # Result = B[i], B[i], A[i], A[i], A[i] \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca5d23f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.554,   0.531,   0.709])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yh = sigmoid(Zh)\n",
    "Yh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d786dd8c",
   "metadata": {},
   "source": [
    "### * **Soft-Max s(z)** \n",
    "Transforma un vector de logits $\\mathbf{z} = (z_1,\\;z_2,\\dots,z_K)$ en un vector de probabilidades $\\mathbf{p} = (p_1,\\;p_2,\\dots,p_K)$:\n",
    "\n",
    "$$\n",
    "\\boxed{%\n",
    "p_k \\;=\\;\n",
    "\\frac{e^{\\,z_k}}\n",
    "     {\\displaystyle\\sum_{j=1}^{K} e^{\\,z_j}}\n",
    "}\n",
    "\\quad\\text{para } k = 1,\\dots,K.\n",
    "$$\n",
    "\n",
    "* Cada $p_k\\in(0,1)$ y $\\sum_{k} p_k = 1$.\n",
    "* En la pr√°ctica se suele restar $\\max_j z_j$ a todos los logits antes de exponenciar para evitar desbordamientos num√©ricos; matem√°ticamente no cambia el resultado porque el factor com√∫n se cancela.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207a6478",
   "metadata": {},
   "source": [
    "    * Understanding matrix operations and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c618a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.554   0.531   0.709]\n",
      "\n",
      "[[ -0.810  -0.176]\n",
      " [ -0.035   0.546]\n",
      " [ -0.113   0.355]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ -0.548,   0.444])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yh = (1, 3)\n",
    "# Who = (2, 3)  -->  Who.T = (3,2)\n",
    "# \n",
    "# Zo = Yh @ Who.T + B0 = (1,2)\n",
    "\n",
    "print(Yh)\n",
    "print()\n",
    "print(Who.T)\n",
    "\n",
    "Zo = Z(Yh, Who, Bo)\n",
    "Zo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11abae88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.578,   1.559])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array(Zo)\n",
    "ezs = np.exp(arr)\n",
    "ezs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8400bda7",
   "metadata": {},
   "source": [
    "    * Keepdims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b62ce288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.000   3.000   4.000   5.000]\n",
      "\n",
      "[[  2.000]\n",
      " [  3.000]\n",
      " [  4.000]\n",
      " [  5.000]]\n"
     ]
    }
   ],
   "source": [
    "Zi = np.array([[ 2. ,  1. , 0. ],\n",
    "              [-1. ,  3. , 2.5],\n",
    "              [ 0.1, -0.2, 4. ],\n",
    "              [ 5. ,  2. , 1. ]])\n",
    "\n",
    "\n",
    "m = np.max(Zi, axis=1)          # ‚Üí shape (4,)\n",
    "print(m)\n",
    "\n",
    "print()\n",
    "m = np.max(Zi, axis=1, keepdims=True)   # ‚Üí shape (4,1)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "857d5228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.371   1.000]\n",
      "[  1.371]\n",
      "[  0.271   0.729]\n"
     ]
    }
   ],
   "source": [
    "# Soft-Max manual\n",
    "Zo_shift = Zo - np.max(Zo, keepdims=True) # Normalizar para evitar overflow del exp\n",
    "\n",
    "\n",
    "ezs = np.exp(Zo_shift)\n",
    "print(ezs)\n",
    "\n",
    "sumezs = np.sum(ezs, keepdims=True)\n",
    "print(sumezs)\n",
    "\n",
    "sf_mx = ezs / sumezs\n",
    "print(sf_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "952e7212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_max(Z):    \n",
    "    ezs = np.exp(Z)\n",
    "\n",
    "    suma =  np.sum(ezs)\n",
    "\n",
    "    result = ezs / suma\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Capaz de tratar vectores 2D, no solo vectores 1D para cuando tratemos con batch\n",
    "def softmax_stable(Z, axis=None):\n",
    "    \"\"\"\n",
    "    Soft-max estable:\n",
    "    - Z puede ser escalar, vector 1-D o matriz 2-D\n",
    "    - axis = None      ‚Üí vector 1-D   (por defecto)\n",
    "      axis = 1         ‚Üí filas de una matriz (batch, clases)\n",
    "    \"\"\"\n",
    "    # 1) Restamos el m√°ximo para evitar overflow de exp\n",
    "    Z_shift = Z - np.max(Z, axis=axis, keepdims=True)\n",
    "\n",
    "    # 2) Exponenciamos\n",
    "    e = np.exp(Z_shift)\n",
    "\n",
    "    # 3) Normalizamos dividiendo por la suma a lo largo del eje elegido, mantenemos la dimensionalidad\n",
    "    sum_e = np.sum(e, axis=axis, keepdims=True)\n",
    "    return e / sum_e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7e2a255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.271,   0.729])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yo = softmax_stable(Zo)\n",
    "Yo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134bd97",
   "metadata": {},
   "source": [
    "### * LOSS FUNCTION: **MSE**\n",
    "\n",
    "La **f√≥rmula del Error Cuadr√°tico Medio (ECM / MSE)** para un conjunto de $n$ observaciones es\n",
    "\n",
    "$$\n",
    "\\text{ECM}\\;=\\;\\frac{1}{n}\\,\\sum_{i=1}^{n}\\bigl(\\hat{o}_i - y_i\\bigr)^2,\n",
    "$$\n",
    "\n",
    "donde\n",
    "\n",
    "* $y_i$‚ÄÉes el valor real (objetivo) de la muestra $i$,\n",
    "* $\\hat{o}_i$‚ÄÇes la predicci√≥n del modelo para esa misma muestra,\n",
    "* $n$‚ÄÉes el n√∫mero total de muestras.\n",
    "\n",
    "\n",
    "Necesitamos una codificacion `One-Hot` para nuestros valores de y esperados, es decir el target:\n",
    "\n",
    "- **0 = [1, 0]**\n",
    "- **1 = [0, 1]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ef7578a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "3    0\n",
       "4    1\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9281ca2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.000,   1.000],\n",
       "       [  1.000,   0.000],\n",
       "       [  0.000,   1.000],\n",
       "       ...,\n",
       "       [  1.000,   0.000],\n",
       "       [  0.000,   1.000],\n",
       "       [  1.000,   0.000]], shape=(768, 2))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ycoded = np.eye(2)[Y]\n",
    "Ycoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c70bc6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be2a130a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.271,   0.729])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16aebbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.07319148013782374)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SME(Y, tgt, axis=None):\n",
    "    \"\"\"\n",
    "    Mean Squared Error:\n",
    "    - axis=None  ‚Üí escalar con el error medio global\n",
    "    - axis=0     ‚Üí medio por columna (clase)  // No se usa // \n",
    "    - axis=1     ‚Üí medio por fila   (ejemplo)\n",
    "    \"\"\"\n",
    "    return np.mean((Y - tgt) ** 2, axis=axis)\n",
    "\n",
    "sme = SME(Yo, Ycoded[0])\n",
    "sme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73bb6e2",
   "metadata": {},
   "source": [
    "### **Cross-Entropy** \n",
    "--> **MSE es mas problematica y menos eficiente**, provoca ajustes m√°s lentos y puede dificultar el aprendizaje, sobre todo cuando ùêæ aumenta.\n",
    "\n",
    "**soft-max + entrop√≠a cruzada** simplifica much√≠simo los c√°lculos y, en clasificaci√≥n, suele aprender mejor que MSE.\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "L \\;=\\; -\\sum_{k=1}^{K} y_k \\,\\log p_k,\n",
    "\\qquad\n",
    "p_k=\\text{softmax}(z_k),\\;\n",
    "y_k\\in\\{0,1\\}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35531f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_entropy(Yo, tgt, axis=None):\n",
    "    \"\"\"\n",
    "    Cross-entropy loss for soft-max outputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Yo   : np.ndarray\n",
    "        Predicted probabilities (output of the soft-max)  \n",
    "        Shape: (*batch*, n_classes) or any shape compatible with `tgt`.\n",
    "\n",
    "    tgt  : np.ndarray\n",
    "        One-hot (or soft) target distribution of the same shape as `Yo`.\n",
    "        \n",
    "    axis : int or None, optional\n",
    "        Dimension along which to sum the class-wise losses.  \n",
    "        ‚Ä¢ None  ‚Üí returns a single scalar (sum over all elements)  \n",
    "        ‚Ä¢ int   ‚Üí returns a loss per slice (e.g. per sample if `axis=1`).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float or np.ndarray\n",
    "        Cross-entropy value(s).\n",
    "    \"\"\"\n",
    "\n",
    "    # -------- estabilidad num√©rica --------------------------------------\n",
    "    eps = 1e-12                  # peque√±o > 0   ‚Üí   evita log(0) = ‚àí‚àû\n",
    "    # np.clip limita Yo al rango [eps, 1]:\n",
    "    #  ‚Ä¢ valores < eps  se elevan a eps\n",
    "    #  ‚Ä¢ valores entre  eps y 1  quedan igual\n",
    "    #  ‚Ä¢ el techo 1 es redundante (soft-max ‚â§ 1) pero inofensivo\n",
    "    safe_p = np.clip(Yo, eps, 1.0)\n",
    "\n",
    "    # -------- p√©rdida ----------------------------------------------------\n",
    "    # F√≥rmula:  L = ‚àí Œ£  y_k ¬∑ log(p_k)\n",
    "    # Se multiplica elemento a elemento y luego se suma en 'axis'\n",
    "    return -np.sum(tgt * np.log(safe_p), axis=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7691338",
   "metadata": {},
   "source": [
    "## **BACKPROPAGATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9569f8",
   "metadata": {},
   "source": [
    "### **Gradiente en la capa de salida**\n",
    "\n",
    "Despu√©s de combinar **la derivada de la p√©rdida con el Jacobiano de la soft-max** sale un resultado sorprendentemente limpio:\n",
    "\n",
    "$$\n",
    "\\boxed{\\displaystyle\n",
    "\\frac{\\partial L}{\\partial z_k}\\;=\\;\\hat{o}_k - y_k\n",
    "}\n",
    "$$\n",
    "\n",
    "Es decir, el **vector** que retro-propagas desde la salida es\n",
    "\n",
    "$$\n",
    "\\delta^{(\\text{salida})} = \\hat{o}_k \\;-\\; \\mathbf{y}.\n",
    "$$\n",
    "\n",
    "#### Ventajas pr√°cticas\n",
    "\n",
    "| Ventaja                     | Por qu√©                                                      |\n",
    "| --------------------------- | ------------------------------------------------------------ |\n",
    "| **F√≥rmula simple**          | No necesitas multiplicaciones adicionales ni sumas cruzadas. |\n",
    "| **Gradiente estable**       | Evita saturaci√≥n cuando $p_k\\to 0$ o $1$.                    |\n",
    "| **Convergencia m√°s r√°pida** | Empuja con m√°s fuerza al principio del entrenamiento.        |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6d16606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_O(tgt, Yo):\n",
    "\n",
    "    grad = Yo - tgt\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b45fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -0.729,  -0.271])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradO = grad_O(Y[0], Yo)\n",
    "gradO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4a157f",
   "metadata": {},
   "source": [
    "#### ¬∑ C√≥mo encaja en tu implementaci√≥n instancia-a-instancia\n",
    "\n",
    "1. **Forward (una fila)**\n",
    "\n",
    "   * $z^{(1)} = W^{(1)}x + b^{(1)}$ ‚Üí ReLU ‚Üí $a^{(1)}$\n",
    "   * $z^{(2)} = W^{(2)}a^{(1)} + b^{(2)}$\n",
    "   * $p = \\text{softmax}(z^{(2)})$\n",
    "\n",
    "2. **P√©rdida**\n",
    "   $L = -\\sum_k y_k\\log p_k$.\n",
    "\n",
    "3. **Delta salida**\n",
    "   $\\delta^{(2)} = p - y$.\n",
    "\n",
    "4. **Delta oculta**\n",
    "   $\\delta^{(1)} = \\bigl(W^{(2)}\\bigr)^{\\!\\top}\\delta^{(2)} \\;\\odot\\; \\mathbf{1}[z^{(1)}>0]$.\n",
    "\n",
    "5. **Gradientes y actualizaci√≥n**\n",
    "\n",
    "   * $\\nabla_{W^{(2)}} = \\delta^{(2)} a^{(1)\\top}$;‚ÄÉ$b^{(2)}\\gets b^{(2)}-\\eta\\,\\delta^{(2)}$\n",
    "   * $\\nabla_{W^{(1)}} = \\delta^{(1)} x^{\\top}$;‚ÄÉ$b^{(1)}\\gets b^{(1)}-\\eta\\,\\delta^{(1)}$\n",
    "\n",
    "*(con tu tasa de aprendizaje $\\eta$)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47694d81",
   "metadata": {},
   "source": [
    "**Gradient at the Hidden Layer**\n",
    "\n",
    "#### **Forma escalar ‚Äì neurona por neurona**\n",
    "\n",
    "$$\n",
    "\\boxed{%\n",
    "\\displaystyle\n",
    "\\delta_i^{(l)}\n",
    "  \\;=\\;\n",
    "\\sigma'\\!\\bigl(z_i^{(l)}\\bigr)\\,\n",
    "\\sum_{k=1}^{d_{l+1}} \n",
    "        w_{ik}^{(l+1)} \\;\\delta_k^{(l+1)}\n",
    "}\\tag{1}\n",
    "$$\n",
    "\n",
    "* $z_i^{(l)}$ ‚ÄÉ‚ÄÉ‚Üí entrada lineal de la neurona $i$ de la capa $l$\n",
    "* $\\sigma'(z)=a(1-a)$ ‚ÄÉ‚ÄÉ‚Üí derivada de la sigmoide en ese punto\n",
    "* $w_{ik}^{(l+1)}$ ‚ÄÉ‚ÄÉ‚Üí peso que va **de la neurona $i$** a la neurona $k$ de la capa $l\\!+\\!1$\n",
    "* $\\delta_k^{(l+1)}$ ‚ÄÉ‚ÄÉ‚Üí culpa ya calculada en la capa siguiente\n",
    "\n",
    "> **Lectura:** ‚ÄúToma la culpa de cada neurona de la capa superior, p√©sala con su conexi√≥n hacia m√≠ y, finalmente, aten√∫a el resultado por la pendiente local de mi sigmoide‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Forma matricial compacta ‚Äì toda la capa de un golpe**\n",
    "\n",
    "$$\n",
    "\\boxed{%\n",
    "\\displaystyle\n",
    "\\delta^{(l)}\n",
    "  = \\bigl(W^{(l+1)}\\bigr)^{\\!\\top} \\,\\delta^{(l+1)}\n",
    "    \\;\\odot\\;\n",
    "    a^{(l)}\\!\\bigl(1-a^{(l)}\\bigr)\n",
    "}\\tag{2}\n",
    "$$\n",
    "\n",
    "* $W^{(l+1)}\\in\\mathbb{R}^{d_{l+1}\\times d_{l}}$\n",
    "  (filas = neuronas de la capa $l\\!+\\!1$, columnas = neuronas de la capa $l$)\n",
    "* $\\delta^{(l)}$, $a^{(l)}$ ‚ÄÉ‚ÄÉ‚Üí vectores de longitud $d_{l}$\n",
    "* $\\odot$ ‚ÄÉ‚ÄÉ‚Üí producto elemento a elemento (Hadamard)\n",
    "\n",
    "---\n",
    "\n",
    "### ¬øPor qu√© usar la versi√≥n matricial?\n",
    "\n",
    "| Punto                        | Escalar (ec. 1)                                                                                         | Matricial (ec. 2)                                                                                                                            |\n",
    "| ---------------------------- | ------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Implementaci√≥n en Python** | Necesita **dos bucles**: uno sobre las neuronas $i$ y otro sobre las neuronas $k$ de la capa siguiente. | Una √∫nica l√≠nea vectorizada (`delta = W.T @ delta_next * (a * (1-a))`).                                                                      |\n",
    "| **Rendimiento**              | Los bucles Python son lentos y no aprovechan BLAS.                                                      | `@` (matmul) llama a BLAS/BLIS/OpenBLAS en C; ejecuta la multiplicaci√≥n en bloque, mucho m√°s r√°pido.                                         |\n",
    "| **Escalabilidad**            | Aumenta cuadr√°ticamente el n¬∫ de operaciones visibles en el c√≥digo.                                     | El mismo c√≥digo funciona para cualquier tama√±o de capa (o incluso un *batch* entero a√±adiendo una dimensi√≥n extra).                          |\n",
    "| **Legibilidad macro**        | Buen para entender la mec√°nica neurona-a-neurona.                                                       | Expresa de golpe la **Regla de la Cadena** para toda la capa: ‚Äúrepartir culpa‚Äù ($W^\\top\\delta$) y ‚Äúmodular por la pendiente‚Äù ($\\sigma'(z)$). |\n",
    "\n",
    "En la pr√°ctica se calcula siempre la forma (2); la forma (1) es la misma operaci√≥n desglosada para que veas exactamente qu√© ocurre en cada neurona.\n",
    "\n",
    "---\n",
    "\n",
    "#### Derivada de la sigmoide\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}, \n",
    "\\qquad\n",
    "\\sigma'(z) = \\sigma(z)\\,\\bigl(1-\\sigma(z)\\bigr) = a\\,(1-a).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "baada4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.158,  -0.030,  -0.003])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grad_H(Yh, gradO):\n",
    "\n",
    "    grad = (Yh*(1-Yh)) * (gradO @ Who)\n",
    "    return grad\n",
    "\n",
    "gradH = grad_H(Yh, gradO)\n",
    "gradH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da0c22",
   "metadata": {},
   "source": [
    "## **Updating W & B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1da58857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector de gradientes capa oculta (gradH):\n",
      "[  0.158  -0.030  -0.003]\n",
      "Shape(gradH):\n",
      "(3,)\n",
      "\n",
      "Vector entrada a la red (Xi):\n",
      "[  0.640   0.848   0.150   0.907  -0.693   0.204   0.468   1.426]\n",
      "Shape(Xi):\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Vector de gradientes capa oculta (gradH):\\n{gradH}')\n",
    "print(f'Shape(gradH):\\n{gradH.shape}')\n",
    "print()\n",
    "print(f'Vector entrada a la red (Xi):\\n{X_1}')\n",
    "print(f'Shape(Xi):\\n{X_1.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4035db4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG\n",
      "[[  0.340  -0.182  -0.133   0.196  -0.368  -0.010   0.528  -0.353]\n",
      " [  0.305  -0.036   0.014   0.616   0.046   0.539  -0.277  -0.387]\n",
      " [ -0.160   0.469  -0.054  -0.745  -0.119   0.060   0.057   0.813]] \n",
      "\n",
      " [[ -0.810  -0.035  -0.113]\n",
      " [ -0.176   0.546   0.355]]\n",
      "\n",
      "Updated\n",
      "[[  0.330  -0.195  -0.135   0.182  -0.357  -0.013   0.520  -0.376]\n",
      " [  0.307  -0.034   0.014   0.619   0.044   0.540  -0.276  -0.383]\n",
      " [ -0.160   0.469  -0.054  -0.744  -0.119   0.060   0.057   0.813]] \n",
      "\n",
      " [[ -0.769   0.003  -0.062]\n",
      " [ -0.161   0.561   0.374]]\n"
     ]
    }
   ],
   "source": [
    "def update_weights(Xi, Wxh, Who, Yh, gradH, gradO):\n",
    "\n",
    "    Wxh = Wxh - (0.1 * np.outer(gradH, Xi))  # No puedo usar @ tal cual porque son dos vectores\n",
    "    Who = Who - (0.1 * np.outer(gradO, Yh))  # planos, no se pueden trasponer como tal.\n",
    "\n",
    "    return Wxh, Who\n",
    "\n",
    "\n",
    "\n",
    "print('OG')\n",
    "print(Wxh, '\\n\\n', Who)\n",
    "\n",
    "Wxh, Who = update_weights(X_1, Wxh, Who, Yh, gradH, gradO)\n",
    "print()\n",
    "print('Updated')\n",
    "print(Wxh, '\\n\\n', Who)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53875861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG\n",
      "[  0.000   0.000   0.000] \n",
      "\n",
      " [  0.000   0.000]\n",
      "\n",
      "Updated\n",
      "[ -0.016   0.003   0.000] \n",
      "\n",
      " [  0.073   0.027]\n"
     ]
    }
   ],
   "source": [
    "def update_bias(Bh, Bo, gradH, gradO):\n",
    "\n",
    "    Bh = Bh - 0.1 * gradH\n",
    "    Bo = Bo - 0.1 * gradO\n",
    "\n",
    "    return Bh, Bo\n",
    "\n",
    "print('OG')\n",
    "print(Bh, '\\n\\n', Bo)\n",
    "\n",
    "Bh, Bo = update_bias(Bh, Bo, gradH, gradO)\n",
    "\n",
    "print()\n",
    "print('Updated')\n",
    "print(Bh, '\\n\\n', Bo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6e4924",
   "metadata": {},
   "source": [
    "# **CLEAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56f9a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "   \n",
    "    def __init__(self, tr_data, tr_target, alpha, epoch):\n",
    "        \n",
    "        self.X = self.standardize(tr_data)\n",
    "        self.target = np.asarray(tr_target, dtype=float)\n",
    "        self.Ycoded = np.eye(2)[self.target]\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.epoch = epoch\n",
    "\n",
    "        self.n_inputs = self.X.shape[1]\n",
    "        self.W = np.random.uniform(-0.5, 0.5, self.n_inputs)\n",
    "        self.U = np.random.uniform(0, 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
