{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d54136",
   "metadata": {},
   "source": [
    "# **MLP** (Multi-Layer Perceptron)\n",
    "<img src=\"docs/images/image-5.png\" alt=\"mlp\" width=\"550\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf38a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as m\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f86e78",
   "metadata": {},
   "source": [
    "## **ARCHITECTURE**\n",
    "\n",
    "* **Entry: 8** attributtes\n",
    "* **Hidden Layers (H): 1**\n",
    "* **Neurons** layer **H: 3**\n",
    "* **Neurons** layer **Out (O) = 2**\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "  <img src=\"docs/images/x1.png\" alt=\"perceptron\" width=\"500\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c81f8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_len = 8\n",
    "H_len = 3\n",
    "O_len = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb940f90",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **OPTIMAL MATRIX LAYOUT**\n",
    "\n",
    "### 1️⃣ **Input**\n",
    "\n",
    "Each input datum (**one instance**):\n",
    "\n",
    "$$\n",
    "x = \\begin{bmatrix} x_1 & x_2 & x_3 & x_4 \\end{bmatrix}_{(1,4)}\n",
    "$$\n",
    "\n",
    "If you use a batch of size $N$:\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{N \\times 4}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ **Hidden layer**\n",
    "\n",
    "#### · **Weights**\n",
    "\n",
    "$$\n",
    "W^{(1)} \\in \\mathbb{R}^{3 \\times 4}\n",
    "$$\n",
    "\n",
    "#### · **Bias**\n",
    "\n",
    "$$\n",
    "b^{(1)} \\in \\mathbb{R}^{1 \\times 3}\n",
    "$$\n",
    "\n",
    "#### · **Pre‑activation computation (z)**\n",
    "\n",
    "$$\n",
    "Z^{(1)} = X \\cdot W^{(1)^T} + b^{(1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^{(1)} \\in \\mathbb{R}^{N \\times 3}\n",
    "$$\n",
    "\n",
    "#### · **Output computation (activation)**\n",
    "\n",
    "$$\n",
    "H^{(1)} = f(Z^{(1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "H^{(1)} \\in \\mathbb{R}^{N \\times 3}\n",
    "$$\n",
    "\n",
    "Here $f$ could be, for example, ReLU or **Sigmoid**, depending on the architecture. \n",
    "\n",
    "---\n",
    "\n",
    "### **3️⃣ Output layer**\n",
    "\n",
    "#### · **Weights**\n",
    "\n",
    "$$\n",
    "W^{(2)} \\in \\mathbb{R}^{2 \\times 3}\n",
    "$$\n",
    "\n",
    "#### · **Bias**\n",
    "\n",
    "$$\n",
    "b^{(2)} \\in \\mathbb{R}^{1 \\times 2}\n",
    "$$\n",
    "\n",
    "#### · **Pre‑activation computation (z)**\n",
    "\n",
    "$$\n",
    "Z^{(2)} = H^{(1)} \\cdot W^{(2)^T} + b^{(2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^{(2)} \\in \\mathbb{R}^{N \\times 2}\n",
    "$$\n",
    "\n",
    "#### · **Final output computation (activation)**\n",
    "\n",
    "$$\n",
    "Y = g(Z^{(2)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y \\in \\mathbb{R}^{N \\times 2}\n",
    "$$\n",
    "\n",
    "Here $g$ could be, for example:\n",
    "\n",
    "* **Softmax if multi‑class classification**\n",
    "* Sigmoid if binary\n",
    "* Identity (no activation) if regression\n",
    "\n",
    "---\n",
    "\n",
    "### Dimension summary\n",
    "\n",
    "| Element                         | Shape  |\n",
    "| ------------------------------- | ------ |\n",
    "| Input $X$                     | (N, 4) |\n",
    "| Weights $W^{(1)}$             | (3, 4) |\n",
    "| Bias $b^{(1)}$                | (1, 3) |\n",
    "| Pre‑activation $Z^{(1)}$      | (N, 3) |\n",
    "| Hidden layer output $H^{(1)}$ | (N, 3) |\n",
    "| Weights $W^{(2)}$             | (2, 3) |\n",
    "| Bias $b^{(2)}$                | (1, 2) |\n",
    "| Pre‑activation $Z^{(2)}$      | (N, 2) |\n",
    "| Final output $Y$              | (N, 2) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7bbe3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **INITIALIZING WHEIGHTS & BIAS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4933517f",
   "metadata": {},
   "source": [
    "### **Glorot (Xavier) Initialization**\n",
    "\n",
    "#### Why the **range and distribution** of the weights matter\n",
    "\n",
    "To avoid the **vanishing or exploding** of weights and gradients, we initialize weights randomly but with care:\n",
    "\n",
    "* `np.random.randn()` returns a **standard normal**: mean 0, variance 1.\n",
    "* If we keep that as‑is and each neuron has many inputs, the weighted sum becomes a **sum of many independent terms**\n",
    "\n",
    "  $$\n",
    "  z \\;=\\; \\sum_{i=1}^{n_{\\text{in}}} w_i\\,x_i\n",
    "  $$\n",
    "\n",
    "  * If both $w_i$ and $x_i$ have variance 1, then\n",
    "    $\\operatorname{Var}(z) = n_{\\text{in}}\\!\\cdot\\!1\\cdot 1 = n_{\\text{in}}$.\n",
    "     With dozens or hundreds of inputs, the **variance of $z$ blows up ⇒ huge activations ⇒ saturated sigmoids/ReLUs ⇒ gradients ≈ 0 (vanishing)**.\n",
    "  * If we go to the other extreme (weights too small), the signal shrinks ⇒ gradients also tiny.\n",
    "\n",
    "The fix is to **force a lower variance** on the weights.\n",
    "Take numbers from $\\mathcal N(0,1)$ and **scale by $\\sqrt{\\text{desired variance}}$**:\n",
    "\n",
    "$$\n",
    "w \\;=\\; \\underbrace{\\mathcal N(0,1)}_{\\text{output of randn}} \\times \\sqrt{\\operatorname{Var}_{\\text{desired}}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "For a layer with\n",
    "\n",
    "* $n\\_{\\text{in}}$ = number of **incoming** units\n",
    "* $n\\_{\\text{out}}$ = number of **outgoing** units\n",
    "\n",
    "set the weight variance to\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(W)=\\frac{2}{n_{\\text{in}}+n_{\\text{out}}}.\n",
    "$$\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\n",
    "w \\;=\\; \\underbrace{\\mathcal N(0,1)}_{\\text{output of randn}} \\times \\sqrt{\\frac{2}{\\,n_{\\text{in}}+n_{\\text{out}}}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Common implementations\n",
    "\n",
    "| Distribution | Sampling rule                                                                                                                               |\n",
    "| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Normal**   | $W\\_{ij}\\sim\\mathcal{N}!\\Bigl(0,;\\dfrac{2}{n\\_{\\text{in}}+n\\_{\\text{out}}}\\Bigr)$                                                         |\n",
    "| **Uniform**  | $W\\_{ij}\\sim\\mathcal U!\\Bigl(,-\\sqrt{\\dfrac{6}{n\\_{\\text{in}}+n\\_{\\text{out}}}},;+\\sqrt{\\dfrac{6}{n\\_{\\text{in}}+n\\_{\\text{out}}}}\\Bigr)$ |\n",
    "\n",
    "> **Python (normal):**\n",
    ">\n",
    "> ```python\n",
    "> W = np.random.randn(n_in, n_out) * np.sqrt(2 / (n_in + n_out))\n",
    "> ```\n",
    ">\n",
    "> **Python (uniform):**\n",
    ">\n",
    "> ```python\n",
    "> limit = np.sqrt(6 / (n_in + n_out))\n",
    "> W = np.random.uniform(-limit, limit, size=(n_in, n_out))\n",
    "> ```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88baeb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for matrix to look good\n",
    "np.set_printoptions(\n",
    "    precision=3,      # decimales\n",
    "    suppress=True,    # evita notación científica\n",
    "    linewidth=120,    # ancho de línea antes de saltar\n",
    "    formatter={'float': '{:7.3f}'.format}  # ancho fijo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bcf52d",
   "metadata": {},
   "source": [
    "### **1. Hidden Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182d358d",
   "metadata": {},
   "source": [
    "#### Weights **W_xh**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7816c68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG\n",
      "[[  0.501  -0.810   0.148   0.768  -0.089  -0.260   0.606  -0.107]\n",
      " [ -0.237  -0.251   0.336   0.056  -0.068   0.129  -0.178   0.166]\n",
      " [ -0.103  -0.577  -0.329   0.633   0.367   0.116   0.136  -0.107]]\n",
      "\n",
      "Transposed\n",
      "[[  0.501  -0.237  -0.103]\n",
      " [ -0.810  -0.251  -0.577]\n",
      " [  0.148   0.336  -0.329]\n",
      " [  0.768   0.056   0.633]\n",
      " [ -0.089  -0.068   0.367]\n",
      " [ -0.260   0.129   0.116]\n",
      " [  0.606  -0.178   0.136]\n",
      " [ -0.107   0.166  -0.107]]\n"
     ]
    }
   ],
   "source": [
    "Wxh = np.random.randn(H_len, entry_len) * np.sqrt(2 / (8 + 3))\n",
    "print('OG')\n",
    "print(Wxh)\n",
    "print('\\nTransposed')\n",
    "print(Wxh.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7b20e",
   "metadata": {},
   "source": [
    "#### Bias **B_h**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f7cdca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.000,   0.000,   0.000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bh = np.zeros(H_len)\n",
    "Bh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d2d578",
   "metadata": {},
   "source": [
    "### **2. Output Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d64e4",
   "metadata": {},
   "source": [
    "#### Weights **W_ho**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23db7fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG\n",
      "[[  0.394   0.701   0.416]\n",
      " [  0.232  -0.486  -0.254]]\n",
      "\n",
      "Transposed\n",
      "[[  0.394   0.232]\n",
      " [  0.701  -0.486]\n",
      " [  0.416  -0.254]]\n"
     ]
    }
   ],
   "source": [
    "Who = np.random.randn(O_len, H_len) * np.sqrt(2 / (3 + 2))\n",
    "\n",
    "print('OG')\n",
    "print(Who)\n",
    "print('\\nTransposed')\n",
    "print(Who.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe07634a",
   "metadata": {},
   "source": [
    "#### Bias **B_o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c8b7d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.000,   0.000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bo = np.zeros(O_len)\n",
    "Bo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc0f6cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We’ll develop the **code incrementally**, **sequentially computing the neural network’s forward pass and back‑propagation on a single instance for clear educational visualization.**”\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3fb2189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raulserrano/Code/ml_projects/perceptron/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"uciml/pima-indians-diabetes-database\")\n",
    "pima = pd.read_csv(f\"{path}/diabetes.csv\")\n",
    "pima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d4bbc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yraw = pima['Outcome'].values\n",
    "Xraw = pima.drop('Outcome', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc37643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "\n",
    "    mu = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    std = np.where(std == 0, 1, std)\n",
    "\n",
    "    XS = (X - mu) / std\n",
    "\n",
    "    return XS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "418f61a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.640   0.848   0.150 ...   0.204   0.468   1.426]\n",
      " [ -0.845  -1.123  -0.161 ...  -0.684  -0.365  -0.191]\n",
      " [  1.234   1.944  -0.264 ...  -1.103   0.604  -0.106]\n",
      " ...\n",
      " [  0.343   0.003   0.150 ...  -0.735  -0.685  -0.276]\n",
      " [ -0.845   0.160  -0.471 ...  -0.240  -0.371   1.171]\n",
      " [ -0.845  -0.873   0.046 ...  -0.202  -0.474  -0.871]]\n",
      "\n",
      "[[  0.000   1.000]\n",
      " [  1.000   0.000]\n",
      " [  0.000   1.000]\n",
      " ...\n",
      " [  1.000   0.000]\n",
      " [  0.000   1.000]\n",
      " [  1.000   0.000]]\n"
     ]
    }
   ],
   "source": [
    "X = standardize(Xraw)\n",
    "Y = np.eye(2)[Yraw]\n",
    "\n",
    "X_1 = X[0]\n",
    "\n",
    "print(X)\n",
    "print()\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a33e498",
   "metadata": {},
   "source": [
    "## **FORMULAS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917642e2",
   "metadata": {},
   "source": [
    "### * **Z**\n",
    "\n",
    "$$\n",
    "Z^{(N)} = X \\cdot W^{(N)^T} + b^{(N)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4a68289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.492,  -0.037,  -0.350])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Xi = (1, 8)\n",
    "Wxh = (3, 8) -->  Wxh.T = (8, 3)\n",
    "Bh = (1, 3)\n",
    "'''\n",
    "#   X_1 @ Wxh = (1,3)\n",
    "\n",
    "def Z(X, W, B):\n",
    "\n",
    "    z = (X @ W.T) + B\n",
    "\n",
    "    return z  \n",
    "\n",
    "Zh = Z(X_1, Wxh, Bh)\n",
    "\n",
    "Zh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e9249e",
   "metadata": {},
   "source": [
    "### \\* **Sigmoid** **σ(z)**\n",
    "\n",
    "$$\n",
    "\\sigma(z)=\\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "**Overflow‑safe equivalent form:**\n",
    "\n",
    "$$\n",
    "\\sigma(z)=\n",
    "\\begin{cases}\n",
    "\\dfrac{1}{1+e^{-z}}, & z\\ge 0,\\\\[8pt]\n",
    "\\dfrac{e^{\\,z}}{1+e^{\\,z}}, & z<0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "* For $z \\ge 0$, the factor $e^{-z}$ is small, so it doesn’t blow up.\n",
    "* For $z < 0$, we use $e^{z}$, which remains tiny (no overflow), and the fraction is algebraically identical to the original sigmoid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79138edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.621,   0.491,   0.413])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# STABLE SIGMOID FORMULA (Perceptron), **scalar Z**\n",
    "def sigmoid_esc(Z):\n",
    "    \"\"\"\n",
    "    Numerically stable sigmoid for a single (scalar) input Z.\n",
    "    Avoids overflow when |Z| is large.\n",
    "    \"\"\"\n",
    "    # f = 1 / (1 + m.exp(-Z))      <-- overflows if Z > ≈700\n",
    "    # return f\n",
    "\n",
    "    if Z >= 0:\n",
    "        return 1 / (1 + m.exp(-Z))\n",
    "    else:\n",
    "        ez = m.exp(Z)              # Z is negative → ez is tiny\n",
    "        return ez / (1 + ez)\n",
    "\n",
    "\n",
    "# STABLE SIGMOID FORMULA, **array Z**\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Numerically stable element‑wise sigmoid for NumPy arrays\n",
    "    (also works for scalars thanks to broadcasting).\n",
    "    \"\"\"\n",
    "\n",
    "    # # 1) Boolean mask for each element\n",
    "    # cond = Z >= 0\n",
    "    #\n",
    "    # # 2) Two expressions, one for Z >= 0 and another for Z < 0\n",
    "    # pos = 1 / (1 + np.exp(-Z))          # ‘positive’ part\n",
    "    # neg = np.exp(Z) / (1 + np.exp(Z))   # ‘negative’ part\n",
    "    #\n",
    "    # # 3) Combine both results according to the mask\n",
    "    # result = np.where(cond, pos, neg)\n",
    "    #\n",
    "    # # 4) Return the array with the sigmoid applied\n",
    "    # return result\n",
    "\n",
    "    result = np.where(\n",
    "        Z >= 0,                      # e.g. [False False  True  True  True]\n",
    "        1 / (1 + np.exp(-Z)),        # Array with the formula for Z >= 0 (A)\n",
    "        np.exp(Z) / (1 + np.exp(Z))  # Array with the formula for Z < 0  (B)\n",
    "    )                                # Result = B[i], B[i], A[i], A[i], A[i]\n",
    "\n",
    "    return result\n",
    "\n",
    "Yh = sigmoid(Zh)\n",
    "Yh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c87bb8",
   "metadata": {},
   "source": [
    "### \\* **Softmax s(z)**\n",
    "\n",
    "Converts a vector of logits $\\mathbf{z}=(z\\_1,;z\\_2,\\dots,z\\_K)$ into a vector of probabilities $\\mathbf{p}=(p\\_1,;p\\_2,\\dots,p\\_K)$:\n",
    "\n",
    "$$\n",
    "\\boxed{%\n",
    "p_k \\;=\\;\n",
    "\\frac{e^{\\,z_k}}\n",
    "     {\\displaystyle\\sum_{j=1}^{K} e^{\\,z_j}}\n",
    "}\n",
    "\\quad\\text{for } k = 1,\\dots,K.\n",
    "$$\n",
    "\n",
    "* Each $p_k$ lies in \\$(0,1)$ and $\\sum\\_k p\\_k = 1$.\n",
    "* In practice, one usually subtracts $\\max_j z_j$ from every logit before exponentiating to avoid numerical overflow. This does not change the final probabilities, because the common factor cancels out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bacd4226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def soft_max(Z):\n",
    "    \"\"\"\n",
    "    Basic soft-max for a 1-D vector (no numerical stabilization).\n",
    "    \"\"\"\n",
    "    ezs = np.exp(Z)\n",
    "\n",
    "    total = np.sum(ezs)   # sum of exponentials\n",
    "\n",
    "    result = ezs / total\n",
    "    return result\n",
    "\n",
    "\n",
    "# Handles 2-D arrays (batches) as well as 1-D vectors\n",
    "def softmax_stable(Z, axis=None):\n",
    "    \"\"\"\n",
    "    Numerically stable soft-max.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Z : scalar, 1-D vector, or 2-D array\n",
    "    axis : int or None, optional\n",
    "        axis=None  → treat as a 1-D vector (default)\n",
    "        axis=1     → rows of a 2-D array (batch x classes)\n",
    "    \"\"\"\n",
    "    # 1) Shift by the maximum to avoid overflow in exp\n",
    "    Z_shift = Z - np.max(Z, axis=axis, keepdims=True)\n",
    "\n",
    "    # 2) Exponentiate the shifted logits\n",
    "    e = np.exp(Z_shift)\n",
    "\n",
    "    # 3) Normalize by the sum along the chosen axis; keep the dimensions\n",
    "    sum_e = np.sum(e, axis=axis, keepdims=True)\n",
    "    return e / sum_e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6045237a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.723,   0.277])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zo = Z(Yh, Who, Bo)\n",
    "Zo\n",
    "\n",
    "Yo = softmax_stable(Zo)\n",
    "Yo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b580d168",
   "metadata": {},
   "source": [
    "### \\* **LOSS FUNCTIONS:** \n",
    "\n",
    "#### **MSE** \n",
    "\n",
    "The **Mean Squared Error (MSE)** for a data set of $n$ observations is\n",
    "\n",
    "$$\n",
    "\\text{MSE}\\;=\\;\\frac{1}{n}\\,\\sum_{i=1}^{n}\\bigl(\\hat{o}_i - y_i\\bigr)^2,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $y_i$ is the true (target) value for sample $i$,\n",
    "* $\\hat{o}_i$ is the model’s prediction for that sample,\n",
    "* $n$ is the total number of samples.\n",
    "\n",
    "For our network the targets must be in **one‑hot** form:\n",
    "\n",
    "* **0 → \\[1, 0]**\n",
    "* **1 → \\[0, 1]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93e4a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SME(Y, tgt, axis=None):\n",
    "    \"\"\"\n",
    "    Mean Squared Error:\n",
    "    - axis=None  → scalar with global mean error\n",
    "    - axis=0     → mean per column (class)  // Not used //\n",
    "    - axis=1     → mean per row (sample)\n",
    "    \"\"\"\n",
    "    return np.mean((Y - tgt) ** 2, axis=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dbda6b",
   "metadata": {},
   "source": [
    "#### **Cross‑Entropy Loss**\n",
    "\n",
    "Mean Squared Error **(MSE)** **often becomes problematic and inefficient** for classification:\n",
    "\n",
    "* With multiple classes ($K>2$) the **gradients** produced by MSE **shrink** as the soft‑max outputs approach the one‑hot targets, so **weight updates slow down**.\n",
    "* Because **MSE treats every output dimension independently**, it **cannot exploit the *probability‑simplex*** structure that naturally couples the class probabilities.\n",
    "\n",
    "By contrast, the **soft‑max + cross‑entropy** combination is almost tailor‑made for classification:\n",
    "\n",
    "* **Clean gradients.**\n",
    "  After applying soft‑max the derivative of the loss `w.r.t.` the logits is simply\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial z_k}=p_k - y_k,\n",
    "  $$\n",
    "\n",
    "  which stays well‑behaved even when $K$ is large.\n",
    "* **Faster convergence.**\n",
    "  The loss penalises confident but wrong predictions much more than uncertain ones, providing a strong corrective signal early in training.\n",
    "* **Probabilistic meaning.**\n",
    "  Minimising cross‑entropy is equivalent to maximising the (log) likelihood of the correct class, giving the output a clear interpretation as class probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "For a single training example with logits $\\mathbf{z}=(z\\_1,\\dots,z\\_K)$ and one‑hot target $\\mathbf{y}=(y\\_1,\\dots,y\\_K)$ the categorical cross‑entropy is\n",
    "\n",
    "$$\n",
    "\\boxed{%\n",
    "L \\;=\\; -\\sum_{k=1}^{K} y_k \\,\\log p_k\n",
    "}\\!,\n",
    "\\qquad\n",
    "p_k=\\operatorname{softmax}(z_k)=\\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}},\n",
    "\\qquad\n",
    "y_k\\in\\{0,1\\}.\n",
    "$$\n",
    "\n",
    "*If $K=2$ this reduces to the familiar binary‑cross‑entropy / log‑loss formula.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10f06a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Yo, tgt, axis=None):\n",
    "    \"\"\"\n",
    "    Cross-entropy loss for soft-max outputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Yo   : np.ndarray\n",
    "        Predicted probabilities (output of the soft-max)  \n",
    "        Shape: (*batch*, n_classes) or any shape compatible with `tgt`.\n",
    "\n",
    "    tgt  : np.ndarray\n",
    "        One-hot (or soft) target distribution of the same shape as `Yo`.\n",
    "        \n",
    "    axis : int or None, optional\n",
    "        Dimension along which to sum the class-wise losses.  \n",
    "        • None  → returns a single scalar (sum over all elements)  \n",
    "        • int   → returns a loss per slice (e.g. per sample if `axis=1`).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float or np.ndarray\n",
    "        Cross-entropy value(s).\n",
    "    \"\"\"\n",
    "\n",
    "    # -------- numerical stability --------------------------------------\n",
    "    eps = 1e-12                  # small > 0  →  avoids log(0) = −∞\n",
    "    # np.clip confines Yo to the range [eps, 1]:\n",
    "    #  • values < eps  are raised to eps\n",
    "    #  • values between eps and 1 remain unchanged\n",
    "    #  • the upper bound 1 is redundant (soft-max ≤ 1) but harmless\n",
    "    safe_p = np.clip(Yo, eps, 1.0)\n",
    "\n",
    "    # -------- loss ------------------------------------------------------\n",
    "    # Formula:  L = − Σ  y_k · log(p_k)\n",
    "    # Multiply element-wise, then sum along the chosen axis\n",
    "    return -np.sum(tgt * np.log(safe_p), axis=axis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff3b7e3",
   "metadata": {},
   "source": [
    "## **BACKPROPAGATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd42b7",
   "metadata": {},
   "source": [
    "### **Gradient at the Output Layer**\n",
    "\n",
    "After you combine **the derivative of the cross‑entropy loss with the Jacobian of the soft‑max**, an unexpectedly tidy result pops out:\n",
    "\n",
    "$$\n",
    "\\boxed{\\displaystyle\n",
    "\\frac{\\partial L}{\\partial z_k}\\;=\\;\\hat{o}_k - y_k\n",
    "}\n",
    "$$\n",
    "\n",
    "Hence, the **vector** you back‑propagate from the output layer is simply\n",
    "\n",
    "$$\n",
    "\\delta^{(\\text{output})}\\;=\\;\\hat{\\mathbf{o}}\\;-\\;\\mathbf{y}.\n",
    "$$\n",
    "\n",
    "#### Practical advantages\n",
    "\n",
    "| Advantage               | Why it matters                                         |\n",
    "| ----------------------- | ------------------------------------------------------ |\n",
    "| **Very simple formula** | No extra multiplications or cross‑sums are required.   |\n",
    "| **Stable gradients**    | Avoids saturation even when $p\\_k \\to 0$ or $1$.   |\n",
    "| **Faster convergence**  | Provides a strong corrective signal early in training. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43793959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.723,  -0.723])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grad_O(tgt, Yo):\n",
    "\n",
    "    grad = Yo - tgt\n",
    "\n",
    "    return grad\n",
    "\n",
    "gradO = grad_O(Y[0], Yo)\n",
    "gradO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf5a40a",
   "metadata": {},
   "source": [
    "### **Gradient at the Hidden Layer**\n",
    "\n",
    "#### **Scalar form – neuron by neuron**\n",
    "\n",
    "$$\n",
    "\\boxed{%\n",
    "\\displaystyle\n",
    "\\delta_i^{(l)}\n",
    "  \\;=\\;\n",
    "\\sigma'\\!\\bigl(z_i^{(l)}\\bigr)\\,\n",
    "\\sum_{k=1}^{d_{l+1}}\n",
    "        w_{ik}^{(l+1)}\\;\\delta_k^{(l+1)}\n",
    "}\\tag{1}\n",
    "$$\n",
    "\n",
    "* $z\\_i^{(l)}$ — linear input to neuron $i$ in layer $l$\n",
    "* $\\sigma'(z)=a,(1-a)$ — derivative of the sigmoid at that point\n",
    "* $w\\_{ik}^{(l+1)}$ — weight **from neuron $i** to neuron $k$ in layer $l!+!1$\n",
    "* $\\delta\\_k^{(l+1)}$ — “blame” (error term) already computed in the next layer\n",
    "\n",
    "> **Intuition:** “Take the error of each neuron in the layer above, weight it by its connection to me, and finally scale the result by my own sigmoid slope.”\n",
    "\n",
    "---\n",
    "\n",
    "#### **Compact matrix form – the whole layer at once**\n",
    "\n",
    "$$\n",
    "\\boxed{%\n",
    "\\displaystyle\n",
    "\\delta^{(l)}\n",
    "  = \\bigl(W^{(l+1)}\\bigr)^{\\!\\top}\\,\\delta^{(l+1)}\n",
    "    \\;\\odot\\;\n",
    "    a^{(l)}\\!\\bigl(1-a^{(l)}\\bigr)\n",
    "}\\tag{2}\n",
    "$$\n",
    "\n",
    "* $W^{(l+1)}!\\in!\\mathbb{R}^{d\\_{l+1}\\times d\\_{l}}$\n",
    "  (rows = neurons in layer $l!+!1$, columns = neurons in layer $l$)\n",
    "* $\\delta^{(l)},;a^{(l)}$ — vectors of length $d\\_{l}$\n",
    "* $\\odot$ — element‑wise (Hadamard) product\n",
    "\n",
    "---\n",
    "\n",
    "### Why prefer the matrix version?\n",
    "\n",
    "| Aspect                    | Scalar (Eq. 1)                                                                                | Matrix (Eq. 2)                                                                                                                                |\n",
    "| ------------------------- | --------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Python implementation** | Needs **two loops**: one over neurons $i$ and another over neurons $k$ in the next layer. | A single vectorised line: `delta = W.T @ delta_next * (a * (1 - a))`.                                                                         |\n",
    "| **Performance**           | Pure‑Python loops are slow and cannot leverage BLAS.                                          | `@` (matrix multiply) dispatches to BLAS/BLIS/OpenBLAS in C, running the block operation far faster.                                          |\n",
    "| **Scalability**           | Code size and visible operations grow quadratically with layer width.                         | Same code works for any layer size—or even a full **batch** by adding one extra dimension.                                                    |\n",
    "| **High‑level clarity**    | Good for seeing the neuron‑to‑neuron mechanics.                                               | Encapsulates the **chain rule** for the entire layer: “redistribute the blame” ($W^\\top\\delta$) then “scale by the slope” ($\\sigma'(z)$). |\n",
    "\n",
    "In practice, Equation (2) is always used; Equation (1) is just the same operation spelled out so you can watch what happens in each individual neuron.\n",
    "\n",
    "---\n",
    "\n",
    "#### Sigmoid derivative reminder\n",
    "\n",
    "$$\n",
    "\\sigma(z)=\\frac{1}{1+e^{-z}}, \n",
    "\\qquad\n",
    "\\sigma'(z)=\\sigma(z)\\,\\bigl(1-\\sigma(z)\\bigr)=a\\,(1-a).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c9fb903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.028,   0.215,   0.117])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grad_H(Yh, gradO):\n",
    "\n",
    "    grad = (Yh*(1-Yh)) * (gradO @ Who)\n",
    "    return grad\n",
    "\n",
    "gradH = grad_H(Yh, gradO)\n",
    "gradH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb4f854",
   "metadata": {},
   "source": [
    "## **Updating W & B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "859bf4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden-layer gradient vector (gradH):\n",
      "[  0.028   0.215   0.117]\n",
      "Shape of gradH:\n",
      "(3,)\n",
      "\n",
      "Input vector to the network (Xi):\n",
      "[  0.640   0.848   0.150   0.907  -0.693   0.204   0.468   1.426]\n",
      "Shape of Xi:\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Hidden-layer gradient vector (gradH):\\n{gradH}')\n",
    "print(f'Shape of gradH:\\n{gradH.shape}')\n",
    "print()\n",
    "print(f'Input vector to the network (Xi):\\n{X_1}')\n",
    "print(f'Shape of Xi:\\n{X_1.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "370ab849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG\n",
      "[[  0.501  -0.810   0.148   0.768  -0.089  -0.260   0.606  -0.107]\n",
      " [ -0.237  -0.251   0.336   0.056  -0.068   0.129  -0.178   0.166]\n",
      " [ -0.103  -0.577  -0.329   0.633   0.367   0.116   0.136  -0.107]] \n",
      "\n",
      " [[  0.394   0.701   0.416]\n",
      " [  0.232  -0.486  -0.254]]\n",
      "\n",
      "Updated\n",
      "[[  0.499  -0.813   0.148   0.765  -0.087  -0.260   0.605  -0.111]\n",
      " [ -0.251  -0.269   0.333   0.036  -0.053   0.125  -0.188   0.136]\n",
      " [ -0.111  -0.587  -0.331   0.622   0.375   0.114   0.130  -0.123]] \n",
      "\n",
      " [[  0.349   0.665   0.386]\n",
      " [  0.277  -0.451  -0.224]]\n"
     ]
    }
   ],
   "source": [
    "def update_weights(Xi, Wxh, Who, Yh, gradH, gradO):\n",
    "\n",
    "    Wxh = Wxh - (0.1 * np.outer(gradH, Xi))  # Can't use @ tal cual because they are plain\n",
    "    Who = Who - (0.1 * np.outer(gradO, Yh))  # 1-D vectors, can't transpose them.\n",
    "\n",
    "    return Wxh, Who\n",
    "\n",
    "\n",
    "\n",
    "print('OG')\n",
    "print(Wxh, '\\n\\n', Who)\n",
    "\n",
    "Wxh, Who = update_weights(X_1, Wxh, Who, Yh, gradH, gradO)\n",
    "print()\n",
    "print('Updated')\n",
    "print(Wxh, '\\n\\n', Who)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41c59ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG\n",
      "[  0.000   0.000   0.000] \n",
      "\n",
      " [  0.000   0.000]\n",
      "\n",
      "Updated\n",
      "[ -0.003  -0.021  -0.012] \n",
      "\n",
      " [ -0.072   0.072]\n"
     ]
    }
   ],
   "source": [
    "def update_bias(Bh, Bo, gradH, gradO):\n",
    "\n",
    "    Bh = Bh - 0.1 * gradH\n",
    "    Bo = Bo - 0.1 * gradO\n",
    "\n",
    "    return Bh, Bo\n",
    "\n",
    "print('OG')\n",
    "print(Bh, '\\n\\n', Bo)\n",
    "\n",
    "Bh, Bo = update_bias(Bh, Bo, gradH, gradO)\n",
    "\n",
    "print()\n",
    "print('Updated')\n",
    "print(Bh, '\\n\\n', Bo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed10c8e",
   "metadata": {},
   "source": [
    "# **CLEAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35487a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "   \n",
    "    def __init__(self, tr_data, tr_target, alpha, epoch):\n",
    "        \n",
    "        self.X = self.standardize(tr_data)\n",
    "        self.target = tr_target\n",
    "        self.Ycoded = np.eye(2)[self.target]\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.epoch = epoch\n",
    "\n",
    "        self.n_inputs = self.X.shape[1]\n",
    "\n",
    "        self.Wxh = np.random.randn(3, self.n_inputs) * np.sqrt(2 / (8 + 3))\n",
    "        self.Who = np.random.randn(2, 3) * np.sqrt(2 / (3 + 2))\n",
    "\n",
    "        self.Bh = np.zeros(3)\n",
    "        self.Bo = np.zeros(2)\n",
    "\n",
    "        self.loss_rows = []\n",
    "        # pd.DataFrame(columns=['epoch', 'loss'])\n",
    "\n",
    "    def standardize(self, X, mode='tr'):\n",
    "\n",
    "        if mode == 'tr': \n",
    "\n",
    "            self.mu = X.mean(axis=0)\n",
    "            self.std = X.std(axis=0)\n",
    "            self.std = np.where(self.std == 0, 1, self.std)\n",
    "\n",
    "            XS = (X - self.mu) / self.std\n",
    "\n",
    "        elif mode == 'ts':\n",
    "        \n",
    "            XS = (X - self.mu) / self.std\n",
    "\n",
    "        return XS\n",
    "\n",
    "    def Z(self, Xi, W, B):\n",
    "        '''\n",
    "        Xi = (1, N)\n",
    "        Wxh = (M, N) -->  Wxh.T = (N, M)\n",
    "        Bh = (1, M)\n",
    "        '''\n",
    "        #   X_1 @ Wxh = (1,M)\n",
    "\n",
    "        z = (Xi @ W.T) + B\n",
    "\n",
    "        return z \n",
    "    \n",
    "\n",
    "    # STABLE SIGMOID FORMULA, **array Z**\n",
    "    def sigmoid(self, Z):\n",
    "        \"\"\"\n",
    "        Numerically stable element-wise sigmoid for NumPy arrays\n",
    "        (also works for scalars thanks to broadcasting).\n",
    "        \"\"\"\n",
    "\n",
    "        # # 1) Boolean mask for each element\n",
    "        # cond = Z >= 0\n",
    "        #\n",
    "        # # 2) Two expressions, one for Z >= 0 and another for Z < 0\n",
    "        # pos = 1 / (1 + np.exp(-Z))          # ‘positive’ part\n",
    "        # neg = np.exp(Z) / (1 + np.exp(Z))   # ‘negative’ part\n",
    "        #\n",
    "        # # 3) Combine both results according to the mask\n",
    "        # result = np.where(cond, pos, neg)\n",
    "        #\n",
    "        # # 4) Return the array with the sigmoid applied\n",
    "        # return result\n",
    "\n",
    "        result = np.where(\n",
    "            Z >= 0,                      # e.g. [False False  True  True  True]\n",
    "            1 / (1 + np.exp(-Z)),        # Array with the formula for Z >= 0 (A)\n",
    "            np.exp(Z) / (1 + np.exp(Z))  # Array with the formula for Z < 0  (B)\n",
    "        )                                # Result = B[i], B[i], A[i], A[i], A[i]\n",
    "\n",
    "        return result\n",
    "    \n",
    "\n",
    "    # Handles 2-D arrays (batches) as well as 1-D vectors\n",
    "    def softmax_stable(self, Z, axis=None):\n",
    "        \"\"\"\n",
    "        Numerically stable soft-max.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : scalar, 1-D vector, or 2-D array\n",
    "        axis : int or None, optional\n",
    "            axis=None  → treat as a 1-D vector (default)\n",
    "            axis=1     → rows of a 2-D array (batch x classes)\n",
    "        \"\"\"\n",
    "        # 1) Shift by the maximum to avoid overflow in exp\n",
    "        Z_shift = Z - np.max(Z, axis=axis, keepdims=True)\n",
    "\n",
    "        # 2) Exponentiate the shifted logits\n",
    "        e = np.exp(Z_shift)\n",
    "\n",
    "        # 3) Normalize by the sum along the chosen axis; keep the dimensions\n",
    "        sum_e = np.sum(e, axis=axis, keepdims=True)\n",
    "        return e / sum_e\n",
    "\n",
    "\n",
    "    def cross_entropy(self, Y, tgt, axis=None):\n",
    "        \"\"\"\n",
    "        Cross-entropy loss for soft-max outputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Yo   : np.ndarray\n",
    "            Predicted probabilities (output of the soft-max)  \n",
    "            Shape: (*batch*, n_classes) or any shape compatible with `tgt`.\n",
    "\n",
    "        tgt  : np.ndarray\n",
    "            One-hot (or soft) target distribution of the same shape as `Yo`.\n",
    "            \n",
    "        axis : int or None, optional\n",
    "            Dimension along which to sum the class-wise losses.  \n",
    "            • None  → returns a single scalar (sum over all elements)  \n",
    "            • int   → returns a loss per slice (e.g. per sample if `axis=1`).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float or np.ndarray\n",
    "            Cross-entropy value(s).\n",
    "        \"\"\"\n",
    "\n",
    "        # -------- numerical stability --------------------------------------\n",
    "        eps = 1e-12                  # small > 0  →  avoids log(0) = −∞\n",
    "        # np.clip confines Yo to the range [eps, 1]:\n",
    "        #  • values < eps  are raised to eps\n",
    "        #  • values between eps and 1 remain unchanged\n",
    "        #  • the upper bound 1 is redundant (soft-max ≤ 1) but harmless\n",
    "        safe_p = np.clip(Y, eps, 1.0)\n",
    "\n",
    "        # -------- loss ------------------------------------------------------\n",
    "        # Formula:  L = − Σ  y_k · log(p_k)\n",
    "        # Multiply element-wise, then sum along the chosen axis\n",
    "        return -np.sum(tgt * np.log(safe_p), axis=axis)\n",
    "    \n",
    "    def grad_O(self, tgt, Y):\n",
    "\n",
    "        grad = Y - tgt\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def grad_H(self, Y):\n",
    "\n",
    "        grad = (Y*(1-Y)) * (self.o_grad @ self.Who)\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def update_weights(self, Xi, Yh):\n",
    "\n",
    "        self.Wxh = self.Wxh - (self.alpha * np.outer(self.h_grad, Xi))  # Can't use @ tal cual because they are plain\n",
    "        self.Who = self.Who - (self.alpha * np.outer(self.o_grad, Yh))  # 1-D vectors, can't transpose them.\n",
    "\n",
    "    def update_bias(self):\n",
    "\n",
    "        self.Bh = self.Bh - self.alpha * self.h_grad\n",
    "        self.Bo = self.Bo - self.alpha * self.o_grad\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for epoch in range(self.epoch):\n",
    "            perm = np.random.permutation(len(self.X))\n",
    "\n",
    "            for i in perm:\n",
    "                Xi = self.X[i]\n",
    "                target_i = self.Ycoded[i]\n",
    "\n",
    "                Zh = self.Z(Xi, self.Wxh, self.Bh)\n",
    "                Yh = self.sigmoid(Zh)          # guardar para pesos salida\n",
    "\n",
    "                Zo = self.Z(Yh, self.Who, self.Bo)\n",
    "                Yo = self.softmax_stable(Zo)\n",
    "\n",
    "                loss_i = self.cross_entropy(Yo, target_i)\n",
    "                self.loss_rows.append((epoch, loss_i))\n",
    "\n",
    "                self.o_grad = self.grad_O(target_i, Yo)\n",
    "                self.h_grad = self.grad_H(Yh)\n",
    "\n",
    "                self.update_weights(Xi, Yh)     # <= pasa Yh\n",
    "                self.update_bias()\n",
    "\n",
    "        self.loss_hist = pd.DataFrame(self.loss_rows, columns=['epoch','loss'])\n",
    "        print('Model trained')\n",
    "\n",
    "    def get_loss(self):\n",
    "        return self.loss_hist\n",
    "\n",
    "    def predict(self, ts_data):\n",
    "\n",
    "        XS = self.standardize(ts_data, mode='ts')\n",
    "\n",
    "        self.results = np.zeros(len(XS), dtype=int)\n",
    "\n",
    "        for i, Xi in enumerate(XS):\n",
    "\n",
    "            Zh = self.Z(Xi, self.Wxh, self.Bh)\n",
    "            Yh = self.sigmoid(Zh)\n",
    "\n",
    "            Zo = self.Z(Yh, self.Who, self.Bo)\n",
    "            Yo = self.softmax_stable(Zo)\n",
    "\n",
    "            self.results[i] = int(Yo[1] >= 0.5)\n",
    "\n",
    "    \n",
    "    def accuracy(self, act_result):\n",
    "\n",
    "        comp = self.results == act_result\n",
    "        accuracy = comp.mean()\n",
    "\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec821cc4",
   "metadata": {},
   "source": [
    "### Function to **split and preprocess data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd1bab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_process(df, prop, tgt_col):\n",
    "    # prop = proportion, range(0.0, 1.0)\n",
    "    # tgt_col = name of the class column\n",
    "\n",
    "    idx = int(len(df) * prop)\n",
    "    \n",
    "    train = df.iloc[0:idx]\n",
    "    test = df.iloc[idx:]\n",
    "\n",
    "    tr_tgt = train[tgt_col].values\n",
    "    train = train.drop(columns=tgt_col)\n",
    "    train = train.values\n",
    "\n",
    "    ts_tgt = test[tgt_col].values\n",
    "    test = test.drop(columns=tgt_col)\n",
    "    test = test.values\n",
    "\n",
    "    return train, tr_tgt, test, ts_tgt\n",
    "\n",
    "train, tr_tgt, test, ts_tgt = split_process(pima, 0.5, 'Outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07f92990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2054c50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.78125)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(train, tr_tgt, 0.1, 20)\n",
    "\n",
    "model.train()\n",
    "model.predict(test)\n",
    "model.accuracy(ts_tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6588c",
   "metadata": {},
   "source": [
    "## **Flexible version** (change Nº neurons in hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebebf37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2:\n",
    "   \n",
    "    def __init__(self, n_hidden, tr_data, tr_target, alpha, epoch):\n",
    "        \n",
    "        self.n_hidden = n_hidden\n",
    "        self.X = self.standardize(tr_data)\n",
    "        self.target = tr_target\n",
    "        self.Ycoded = np.eye(2)[self.target]\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.epoch = epoch\n",
    "\n",
    "        self.n_inputs = self.X.shape[1]\n",
    "\n",
    "        self.Wxh = np.random.randn(self.n_hidden, self.n_inputs) * np.sqrt(2 / (self.n_inputs + self.n_hidden))\n",
    "        self.Who = np.random.randn(2, self.n_hidden) * np.sqrt(2 / (self.n_hidden + 2))\n",
    "\n",
    "        self.Bh = np.zeros(n_hidden)\n",
    "        self.Bo = np.zeros(2)\n",
    "\n",
    "        self.loss_rows = []\n",
    "\n",
    "    def standardize(self, X, mode='tr'):\n",
    "\n",
    "        if mode == 'tr': \n",
    "\n",
    "            self.mu = X.mean(axis=0)\n",
    "            self.std = X.std(axis=0)\n",
    "            self.std = np.where(self.std == 0, 1, self.std)\n",
    "\n",
    "            XS = (X - self.mu) / self.std\n",
    "\n",
    "        elif mode == 'ts':\n",
    "        \n",
    "            XS = (X - self.mu) / self.std\n",
    "\n",
    "        return XS\n",
    "\n",
    "    def Z(self, Xi, W, B):\n",
    "        '''\n",
    "        Xi = (1, N)\n",
    "        Wxh = (M, N) -->  Wxh.T = (N, M)\n",
    "        Bh = (1, M)\n",
    "        '''\n",
    "        #   X_1 @ Wxh = (1,M)\n",
    "\n",
    "        z = (Xi @ W.T) + B\n",
    "\n",
    "        return z \n",
    "    \n",
    "\n",
    "    # STABLE SIGMOID FORMULA, **array Z**\n",
    "    def sigmoid(self, Z):\n",
    "        \"\"\"\n",
    "        Numerically stable element-wise sigmoid for NumPy arrays\n",
    "        (also works for scalars thanks to broadcasting).\n",
    "        \"\"\"\n",
    "\n",
    "        # # 1) Boolean mask for each element\n",
    "        # cond = Z >= 0\n",
    "        #\n",
    "        # # 2) Two expressions, one for Z >= 0 and another for Z < 0\n",
    "        # pos = 1 / (1 + np.exp(-Z))          # ‘positive’ part\n",
    "        # neg = np.exp(Z) / (1 + np.exp(Z))   # ‘negative’ part\n",
    "        #\n",
    "        # # 3) Combine both results according to the mask\n",
    "        # result = np.where(cond, pos, neg)\n",
    "        #\n",
    "        # # 4) Return the array with the sigmoid applied\n",
    "        # return result\n",
    "\n",
    "        result = np.where(\n",
    "            Z >= 0,                      # e.g. [False False  True  True  True]\n",
    "            1 / (1 + np.exp(-Z)),        # Array with the formula for Z >= 0 (A)\n",
    "            np.exp(Z) / (1 + np.exp(Z))  # Array with the formula for Z < 0  (B)\n",
    "        )                                # Result = B[i], B[i], A[i], A[i], A[i]\n",
    "\n",
    "        return result\n",
    "    \n",
    "\n",
    "    # Handles 2-D arrays (batches) as well as 1-D vectors\n",
    "    def softmax_stable(self, Z, axis=None):\n",
    "        \"\"\"\n",
    "        Numerically stable soft-max.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : scalar, 1-D vector, or 2-D array\n",
    "        axis : int or None, optional\n",
    "            axis=None  → treat as a 1-D vector (default)\n",
    "            axis=1     → rows of a 2-D array (batch x classes)\n",
    "        \"\"\"\n",
    "        # 1) Shift by the maximum to avoid overflow in exp\n",
    "        Z_shift = Z - np.max(Z, axis=axis, keepdims=True)\n",
    "\n",
    "        # 2) Exponentiate the shifted logits\n",
    "        e = np.exp(Z_shift)\n",
    "\n",
    "        # 3) Normalize by the sum along the chosen axis; keep the dimensions\n",
    "        sum_e = np.sum(e, axis=axis, keepdims=True)\n",
    "        return e / sum_e\n",
    "\n",
    "\n",
    "    def cross_entropy(self, Y, tgt, axis=None):\n",
    "        \"\"\"\n",
    "        Cross-entropy loss for soft-max outputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Yo   : np.ndarray\n",
    "            Predicted probabilities (output of the soft-max)  \n",
    "            Shape: (*batch*, n_classes) or any shape compatible with `tgt`.\n",
    "\n",
    "        tgt  : np.ndarray\n",
    "            One-hot (or soft) target distribution of the same shape as `Yo`.\n",
    "            \n",
    "        axis : int or None, optional\n",
    "            Dimension along which to sum the class-wise losses.  \n",
    "            • None  → returns a single scalar (sum over all elements)  \n",
    "            • int   → returns a loss per slice (e.g. per sample if `axis=1`).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float or np.ndarray\n",
    "            Cross-entropy value(s).\n",
    "        \"\"\"\n",
    "\n",
    "        # -------- numerical stability --------------------------------------\n",
    "        eps = 1e-12                  # small > 0  →  avoids log(0) = −∞\n",
    "        # np.clip confines Yo to the range [eps, 1]:\n",
    "        #  • values < eps  are raised to eps\n",
    "        #  • values between eps and 1 remain unchanged\n",
    "        #  • the upper bound 1 is redundant (soft-max ≤ 1) but harmless\n",
    "        safe_p = np.clip(Y, eps, 1.0)\n",
    "\n",
    "        # -------- loss ------------------------------------------------------\n",
    "        # Formula:  L = − Σ  y_k · log(p_k)\n",
    "        # Multiply element-wise, then sum along the chosen axis\n",
    "        return -np.sum(tgt * np.log(safe_p), axis=axis)\n",
    "    \n",
    "    def grad_O(self, tgt, Y):\n",
    "\n",
    "        grad = Y - tgt\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def grad_H(self, Y):\n",
    "\n",
    "        grad = (Y*(1-Y)) * (self.o_grad @ self.Who)\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def update_weights(self, Xi, Yh):\n",
    "\n",
    "        self.Wxh = self.Wxh - (self.alpha * np.outer(self.h_grad, Xi))  # Can't use @ tal cual because they are plain\n",
    "        self.Who = self.Who - (self.alpha * np.outer(self.o_grad, Yh))  # 1-D vectors, can't transpose them.\n",
    "\n",
    "    def update_bias(self):\n",
    "\n",
    "        self.Bh = self.Bh - self.alpha * self.h_grad\n",
    "        self.Bo = self.Bo - self.alpha * self.o_grad\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for epoch in range(self.epoch):\n",
    "            perm = np.random.permutation(len(self.X))\n",
    "\n",
    "            for i in perm:\n",
    "                Xi = self.X[i]\n",
    "                target_i = self.Ycoded[i]\n",
    "\n",
    "                Zh = self.Z(Xi, self.Wxh, self.Bh)\n",
    "                Yh = self.sigmoid(Zh)          # guardar para pesos salida\n",
    "\n",
    "                Zo = self.Z(Yh, self.Who, self.Bo)\n",
    "                Yo = self.softmax_stable(Zo)\n",
    "\n",
    "                loss_i = self.cross_entropy(Yo, target_i)\n",
    "                self.loss_rows.append((epoch, loss_i))\n",
    "\n",
    "                self.o_grad = self.grad_O(target_i, Yo)\n",
    "                self.h_grad = self.grad_H(Yh)\n",
    "\n",
    "                self.update_weights(Xi, Yh)     # <= pasa Yh\n",
    "                self.update_bias()\n",
    "\n",
    "        self.loss_hist = pd.DataFrame(self.loss_rows, columns=['epoch','loss'])\n",
    "        print('Model trained')\n",
    "\n",
    "    def get_loss(self):\n",
    "        return self.loss_hist\n",
    "\n",
    "    def predict(self, ts_data):\n",
    "\n",
    "        XS = self.standardize(ts_data, mode='ts')\n",
    "\n",
    "        self.results = np.zeros(len(XS), dtype=int)\n",
    "\n",
    "        for i, Xi in enumerate(XS):\n",
    "\n",
    "            Zh = self.Z(Xi, self.Wxh, self.Bh)\n",
    "            Yh = self.sigmoid(Zh)\n",
    "\n",
    "            Zo = self.Z(Yh, self.Who, self.Bo)\n",
    "            Yo = self.softmax_stable(Zo)\n",
    "\n",
    "            self.results[i] = int(Yo[1] >= 0.5)\n",
    "\n",
    "    \n",
    "    def accuracy(self, act_result):\n",
    "\n",
    "        comp = self.results == act_result\n",
    "        accuracy = comp.mean()\n",
    "\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "855870e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.8125)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP2(16,train, tr_tgt, 0.1, 20)\n",
    "\n",
    "model.train()\n",
    "model.predict(test)\n",
    "model.accuracy(ts_tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1aad34",
   "metadata": {},
   "source": [
    "### **Test on other dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2a86674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data   = load_breast_cancer()\n",
    "data\n",
    "X_full = data.data                  # shape (569, 30)\n",
    "y_full = data.target.astype(int)    # 0 = benign, 1 = malignant\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_full, y_full, test_size=0.4, random_state=42, stratify=y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "440c92fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained\n",
      "Test accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP2(n_hidden=8, tr_data=X_tr, tr_target=y_tr,\n",
    "           alpha=0.05, epoch=50)\n",
    "mlp.train()\n",
    "mlp.predict(X_te)\n",
    "\n",
    "print(\"Test accuracy:\", mlp.accuracy(y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6491927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 17.990,  10.380, 122.800, ...,   0.265,   0.460,   0.119],\n",
       "       [ 20.570,  17.770, 132.900, ...,   0.186,   0.275,   0.089],\n",
       "       [ 19.690,  21.250, 130.000, ...,   0.243,   0.361,   0.088],\n",
       "       ...,\n",
       "       [ 16.600,  28.080, 108.300, ...,   0.142,   0.222,   0.078],\n",
       "       [ 20.600,  29.330, 140.100, ...,   0.265,   0.409,   0.124],\n",
       "       [  7.760,  24.540,  47.920, ...,   0.000,   0.287,   0.070]], shape=(569, 30))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
